{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from train import train_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 2, 64, 32]               8\n",
      "       BatchNorm2d-2            [-1, 2, 64, 32]               4\n",
      "               ELU-3            [-1, 2, 64, 32]               0\n",
      "            Conv2d-4            [-1, 4, 32, 32]              28\n",
      "       BatchNorm2d-5            [-1, 4, 32, 32]               8\n",
      "               ELU-6            [-1, 4, 32, 32]               0\n",
      "            Conv2d-7            [-1, 8, 32, 16]             104\n",
      "       BatchNorm2d-8            [-1, 8, 32, 16]              16\n",
      "               ELU-9            [-1, 8, 32, 16]               0\n",
      "           Conv2d-10           [-1, 16, 16, 16]             400\n",
      "      BatchNorm2d-11           [-1, 16, 16, 16]              32\n",
      "              ELU-12           [-1, 16, 16, 16]               0\n",
      "           Conv2d-13            [-1, 32, 16, 8]           1,568\n",
      "      BatchNorm2d-14            [-1, 32, 16, 8]              64\n",
      "              ELU-15            [-1, 32, 16, 8]               0\n",
      "           Conv2d-16             [-1, 64, 8, 8]           6,208\n",
      "      BatchNorm2d-17             [-1, 64, 8, 8]             128\n",
      "              ELU-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 8, 4]          24,704\n",
      "      BatchNorm2d-20            [-1, 128, 8, 4]             256\n",
      "              ELU-21            [-1, 128, 8, 4]               0\n",
      "           Conv2d-22            [-1, 256, 4, 4]          98,560\n",
      "      BatchNorm2d-23            [-1, 256, 4, 4]             512\n",
      "              ELU-24            [-1, 256, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 2]         393,728\n",
      "      BatchNorm2d-26            [-1, 512, 4, 2]           1,024\n",
      "              ELU-27            [-1, 512, 4, 2]               0\n",
      "           Conv2d-28           [-1, 1024, 2, 2]       1,573,888\n",
      "      BatchNorm2d-29           [-1, 1024, 2, 2]           2,048\n",
      "              ELU-30           [-1, 1024, 2, 2]               0\n",
      "AdaptiveAvgPool2d-31           [-1, 1024, 1, 1]               0\n",
      "          Flatten-32                 [-1, 1024]               0\n",
      "           Linear-33                   [-1, 10]             170\n",
      "      BatchNorm1d-34                   [-1, 10]              20\n",
      "              ELU-35                   [-1, 10]               0\n",
      "           Linear-36                    [-1, 1]              11\n",
      "           Linear-37                   [-1, 10]             170\n",
      "      BatchNorm1d-38                   [-1, 10]              20\n",
      "              ELU-39                   [-1, 10]               0\n",
      "           Linear-40                    [-1, 1]              11\n",
      "           Linear-41                   [-1, 10]             170\n",
      "      BatchNorm1d-42                   [-1, 10]              20\n",
      "              ELU-43                   [-1, 10]               0\n",
      "           Linear-44                    [-1, 1]              11\n",
      "           Linear-45                   [-1, 10]             170\n",
      "      BatchNorm1d-46                   [-1, 10]              20\n",
      "              ELU-47                   [-1, 10]               0\n",
      "           Linear-48                    [-1, 1]              11\n",
      "           Linear-49                   [-1, 10]             170\n",
      "      BatchNorm1d-50                   [-1, 10]              20\n",
      "              ELU-51                   [-1, 10]               0\n",
      "           Linear-52                    [-1, 1]              11\n",
      "           Linear-53                   [-1, 10]             170\n",
      "      BatchNorm1d-54                   [-1, 10]              20\n",
      "              ELU-55                   [-1, 10]               0\n",
      "           Linear-56                    [-1, 1]              11\n",
      "           Linear-57                   [-1, 10]             170\n",
      "      BatchNorm1d-58                   [-1, 10]              20\n",
      "              ELU-59                   [-1, 10]               0\n",
      "           Linear-60                    [-1, 1]              11\n",
      "           Linear-61                   [-1, 10]             170\n",
      "      BatchNorm1d-62                   [-1, 10]              20\n",
      "              ELU-63                   [-1, 10]               0\n",
      "           Linear-64                    [-1, 1]              11\n",
      "           Linear-65                   [-1, 10]             170\n",
      "      BatchNorm1d-66                   [-1, 10]              20\n",
      "              ELU-67                   [-1, 10]               0\n",
      "           Linear-68                    [-1, 1]              11\n",
      "           Linear-69                   [-1, 10]             170\n",
      "      BatchNorm1d-70                   [-1, 10]              20\n",
      "              ELU-71                   [-1, 10]               0\n",
      "           Linear-72                    [-1, 1]              11\n",
      "           Linear-73                   [-1, 10]             170\n",
      "      BatchNorm1d-74                   [-1, 10]              20\n",
      "              ELU-75                   [-1, 10]               0\n",
      "           Linear-76                    [-1, 1]              11\n",
      "           Linear-77                   [-1, 10]             170\n",
      "      BatchNorm1d-78                   [-1, 10]              20\n",
      "              ELU-79                   [-1, 10]               0\n",
      "           Linear-80                    [-1, 1]              11\n",
      "           Linear-81                   [-1, 10]             170\n",
      "      BatchNorm1d-82                   [-1, 10]              20\n",
      "              ELU-83                   [-1, 10]               0\n",
      "           Linear-84                    [-1, 1]              11\n",
      "           Linear-85                   [-1, 10]             170\n",
      "      BatchNorm1d-86                   [-1, 10]              20\n",
      "              ELU-87                   [-1, 10]               0\n",
      "           Linear-88                    [-1, 1]              11\n",
      "           Linear-89                   [-1, 10]             170\n",
      "      BatchNorm1d-90                   [-1, 10]              20\n",
      "              ELU-91                   [-1, 10]               0\n",
      "           Linear-92                    [-1, 1]              11\n",
      "           Linear-93                   [-1, 10]             170\n",
      "      BatchNorm1d-94                   [-1, 10]              20\n",
      "              ELU-95                   [-1, 10]               0\n",
      "           Linear-96                    [-1, 1]              11\n",
      "           Linear-97                   [-1, 10]             170\n",
      "      BatchNorm1d-98                   [-1, 10]              20\n",
      "              ELU-99                   [-1, 10]               0\n",
      "          Linear-100                    [-1, 1]              11\n",
      "          Linear-101                   [-1, 10]             170\n",
      "     BatchNorm1d-102                   [-1, 10]              20\n",
      "             ELU-103                   [-1, 10]               0\n",
      "          Linear-104                    [-1, 1]              11\n",
      "          Linear-105                   [-1, 10]             170\n",
      "     BatchNorm1d-106                   [-1, 10]              20\n",
      "             ELU-107                   [-1, 10]               0\n",
      "          Linear-108                    [-1, 1]              11\n",
      "          Linear-109                   [-1, 10]             170\n",
      "     BatchNorm1d-110                   [-1, 10]              20\n",
      "             ELU-111                   [-1, 10]               0\n",
      "          Linear-112                    [-1, 1]              11\n",
      "          Linear-113                   [-1, 10]             170\n",
      "     BatchNorm1d-114                   [-1, 10]              20\n",
      "             ELU-115                   [-1, 10]               0\n",
      "          Linear-116                    [-1, 1]              11\n",
      "          Linear-117                   [-1, 10]             170\n",
      "     BatchNorm1d-118                   [-1, 10]              20\n",
      "             ELU-119                   [-1, 10]               0\n",
      "          Linear-120                    [-1, 1]              11\n",
      "          Linear-121                   [-1, 10]             170\n",
      "     BatchNorm1d-122                   [-1, 10]              20\n",
      "             ELU-123                   [-1, 10]               0\n",
      "          Linear-124                    [-1, 1]              11\n",
      "          Linear-125                   [-1, 10]             170\n",
      "     BatchNorm1d-126                   [-1, 10]              20\n",
      "             ELU-127                   [-1, 10]               0\n",
      "          Linear-128                    [-1, 1]              11\n",
      "          Linear-129                   [-1, 10]             170\n",
      "     BatchNorm1d-130                   [-1, 10]              20\n",
      "             ELU-131                   [-1, 10]               0\n",
      "          Linear-132                    [-1, 1]              11\n",
      "          Linear-133                   [-1, 10]             170\n",
      "     BatchNorm1d-134                   [-1, 10]              20\n",
      "             ELU-135                   [-1, 10]               0\n",
      "          Linear-136                    [-1, 1]              11\n",
      "          Linear-137                   [-1, 10]             170\n",
      "     BatchNorm1d-138                   [-1, 10]              20\n",
      "             ELU-139                   [-1, 10]               0\n",
      "          Linear-140                    [-1, 1]              11\n",
      "          Linear-141                   [-1, 10]             170\n",
      "     BatchNorm1d-142                   [-1, 10]              20\n",
      "             ELU-143                   [-1, 10]               0\n",
      "          Linear-144                    [-1, 1]              11\n",
      "          Linear-145                   [-1, 10]             170\n",
      "     BatchNorm1d-146                   [-1, 10]              20\n",
      "             ELU-147                   [-1, 10]               0\n",
      "          Linear-148                    [-1, 1]              11\n",
      "          Linear-149                   [-1, 10]             170\n",
      "     BatchNorm1d-150                   [-1, 10]              20\n",
      "             ELU-151                   [-1, 10]               0\n",
      "          Linear-152                    [-1, 1]              11\n",
      "          Linear-153                   [-1, 10]             170\n",
      "     BatchNorm1d-154                   [-1, 10]              20\n",
      "             ELU-155                   [-1, 10]               0\n",
      "          Linear-156                    [-1, 1]              11\n",
      "          Linear-157                   [-1, 10]             170\n",
      "     BatchNorm1d-158                   [-1, 10]              20\n",
      "             ELU-159                   [-1, 10]               0\n",
      "          Linear-160                    [-1, 1]              11\n",
      "          Linear-161                   [-1, 10]             170\n",
      "     BatchNorm1d-162                   [-1, 10]              20\n",
      "             ELU-163                   [-1, 10]               0\n",
      "          Linear-164                    [-1, 1]              11\n",
      "          Linear-165                   [-1, 10]             170\n",
      "     BatchNorm1d-166                   [-1, 10]              20\n",
      "             ELU-167                   [-1, 10]               0\n",
      "          Linear-168                    [-1, 1]              11\n",
      "          Linear-169                   [-1, 10]             170\n",
      "     BatchNorm1d-170                   [-1, 10]              20\n",
      "             ELU-171                   [-1, 10]               0\n",
      "          Linear-172                    [-1, 1]              11\n",
      "          Linear-173                   [-1, 10]             170\n",
      "     BatchNorm1d-174                   [-1, 10]              20\n",
      "             ELU-175                   [-1, 10]               0\n",
      "          Linear-176                    [-1, 1]              11\n",
      "          Linear-177                   [-1, 10]             170\n",
      "     BatchNorm1d-178                   [-1, 10]              20\n",
      "             ELU-179                   [-1, 10]               0\n",
      "          Linear-180                    [-1, 1]              11\n",
      "          Linear-181                   [-1, 10]             170\n",
      "     BatchNorm1d-182                   [-1, 10]              20\n",
      "             ELU-183                   [-1, 10]               0\n",
      "          Linear-184                    [-1, 1]              11\n",
      "          Linear-185                   [-1, 10]             170\n",
      "     BatchNorm1d-186                   [-1, 10]              20\n",
      "             ELU-187                   [-1, 10]               0\n",
      "          Linear-188                    [-1, 1]              11\n",
      "          Linear-189                   [-1, 10]             170\n",
      "     BatchNorm1d-190                   [-1, 10]              20\n",
      "             ELU-191                   [-1, 10]               0\n",
      "          Linear-192                    [-1, 1]              11\n",
      "          Linear-193                   [-1, 10]             170\n",
      "     BatchNorm1d-194                   [-1, 10]              20\n",
      "             ELU-195                   [-1, 10]               0\n",
      "          Linear-196                    [-1, 1]              11\n",
      "          Linear-197                   [-1, 10]             170\n",
      "     BatchNorm1d-198                   [-1, 10]              20\n",
      "             ELU-199                   [-1, 10]               0\n",
      "          Linear-200                    [-1, 1]              11\n",
      "          Linear-201                   [-1, 10]             170\n",
      "     BatchNorm1d-202                   [-1, 10]              20\n",
      "             ELU-203                   [-1, 10]               0\n",
      "          Linear-204                    [-1, 1]              11\n",
      "          Linear-205                   [-1, 10]             170\n",
      "     BatchNorm1d-206                   [-1, 10]              20\n",
      "             ELU-207                   [-1, 10]               0\n",
      "          Linear-208                    [-1, 1]              11\n",
      "          Linear-209                   [-1, 10]             170\n",
      "     BatchNorm1d-210                   [-1, 10]              20\n",
      "             ELU-211                   [-1, 10]               0\n",
      "          Linear-212                    [-1, 1]              11\n",
      "          Linear-213                   [-1, 10]             170\n",
      "     BatchNorm1d-214                   [-1, 10]              20\n",
      "             ELU-215                   [-1, 10]               0\n",
      "          Linear-216                    [-1, 1]              11\n",
      "          Linear-217                   [-1, 10]             170\n",
      "     BatchNorm1d-218                   [-1, 10]              20\n",
      "             ELU-219                   [-1, 10]               0\n",
      "          Linear-220                    [-1, 1]              11\n",
      "          Linear-221                   [-1, 10]             170\n",
      "     BatchNorm1d-222                   [-1, 10]              20\n",
      "             ELU-223                   [-1, 10]               0\n",
      "          Linear-224                    [-1, 1]              11\n",
      "          Linear-225                   [-1, 10]             170\n",
      "     BatchNorm1d-226                   [-1, 10]              20\n",
      "             ELU-227                   [-1, 10]               0\n",
      "          Linear-228                    [-1, 1]              11\n",
      "          Linear-229                   [-1, 10]             170\n",
      "     BatchNorm1d-230                   [-1, 10]              20\n",
      "             ELU-231                   [-1, 10]               0\n",
      "          Linear-232                    [-1, 1]              11\n",
      "          Linear-233                   [-1, 10]             170\n",
      "     BatchNorm1d-234                   [-1, 10]              20\n",
      "             ELU-235                   [-1, 10]               0\n",
      "          Linear-236                    [-1, 1]              11\n",
      "          Linear-237                   [-1, 10]             170\n",
      "     BatchNorm1d-238                   [-1, 10]              20\n",
      "             ELU-239                   [-1, 10]               0\n",
      "          Linear-240                    [-1, 1]              11\n",
      "          Linear-241                   [-1, 10]             170\n",
      "     BatchNorm1d-242                   [-1, 10]              20\n",
      "             ELU-243                   [-1, 10]               0\n",
      "          Linear-244                    [-1, 1]              11\n",
      "          Linear-245                   [-1, 10]             170\n",
      "     BatchNorm1d-246                   [-1, 10]              20\n",
      "             ELU-247                   [-1, 10]               0\n",
      "          Linear-248                    [-1, 1]              11\n",
      "          Linear-249                   [-1, 10]             170\n",
      "     BatchNorm1d-250                   [-1, 10]              20\n",
      "             ELU-251                   [-1, 10]               0\n",
      "          Linear-252                    [-1, 1]              11\n",
      "          Linear-253                   [-1, 10]             170\n",
      "     BatchNorm1d-254                   [-1, 10]              20\n",
      "             ELU-255                   [-1, 10]               0\n",
      "          Linear-256                    [-1, 1]              11\n",
      "          Linear-257                   [-1, 10]             170\n",
      "     BatchNorm1d-258                   [-1, 10]              20\n",
      "             ELU-259                   [-1, 10]               0\n",
      "          Linear-260                    [-1, 1]              11\n",
      "          Linear-261                   [-1, 10]             170\n",
      "     BatchNorm1d-262                   [-1, 10]              20\n",
      "             ELU-263                   [-1, 10]               0\n",
      "          Linear-264                    [-1, 1]              11\n",
      "          Linear-265                   [-1, 10]             170\n",
      "     BatchNorm1d-266                   [-1, 10]              20\n",
      "             ELU-267                   [-1, 10]               0\n",
      "          Linear-268                    [-1, 1]              11\n",
      "          Linear-269                   [-1, 10]             170\n",
      "     BatchNorm1d-270                   [-1, 10]              20\n",
      "             ELU-271                   [-1, 10]               0\n",
      "          Linear-272                    [-1, 1]              11\n",
      "          Linear-273                   [-1, 10]             170\n",
      "     BatchNorm1d-274                   [-1, 10]              20\n",
      "             ELU-275                   [-1, 10]               0\n",
      "          Linear-276                    [-1, 1]              11\n",
      "          Linear-277                   [-1, 10]             170\n",
      "     BatchNorm1d-278                   [-1, 10]              20\n",
      "             ELU-279                   [-1, 10]               0\n",
      "          Linear-280                    [-1, 1]              11\n",
      "          Linear-281                   [-1, 10]             170\n",
      "     BatchNorm1d-282                   [-1, 10]              20\n",
      "             ELU-283                   [-1, 10]               0\n",
      "          Linear-284                    [-1, 1]              11\n",
      "          Linear-285                   [-1, 10]             170\n",
      "     BatchNorm1d-286                   [-1, 10]              20\n",
      "             ELU-287                   [-1, 10]               0\n",
      "          Linear-288                    [-1, 1]              11\n",
      "================================================================\n",
      "Total params: 2,116,152\n",
      "Trainable params: 2,116,152\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.97\n",
      "Params size (MB): 8.07\n",
      "Estimated Total Size (MB): 9.06\n",
      "----------------------------------------------------------------\n",
      "Learning started\n",
      "    Batch 0 of 468 loss: 0.6929787397384644\n",
      "    Batch 1 of 468 loss: 0.5417003035545349\n",
      "    Batch 2 of 468 loss: 0.3100629150867462\n",
      "    Batch 3 of 468 loss: 0.20136791467666626\n",
      "    Batch 4 of 468 loss: 0.09943599998950958\n",
      "    Batch 5 of 468 loss: 0.06090986728668213\n",
      "    Batch 6 of 468 loss: 0.027845310047268867\n",
      "    Batch 7 of 468 loss: 0.04375491291284561\n",
      "    Batch 8 of 468 loss: 0.002625899389386177\n",
      "    Batch 9 of 468 loss: 0.02970760315656662\n",
      "    Batch 10 of 468 loss: 0.009790923446416855\n",
      "    Batch 11 of 468 loss: 0.0\n",
      "    Batch 12 of 468 loss: 0.007817881181836128\n",
      "    Batch 13 of 468 loss: 0.008968949317932129\n",
      "    Batch 14 of 468 loss: 0.0010063610970973969\n",
      "    Batch 15 of 468 loss: 0.014946237206459045\n",
      "    Batch 16 of 468 loss: 0.0031896475702524185\n",
      "    Batch 17 of 468 loss: 0.016174884513020515\n",
      "    Batch 18 of 468 loss: 0.007314968854188919\n",
      "    Batch 19 of 468 loss: 0.001810327172279358\n",
      "    Batch 20 of 468 loss: 0.007586397230625153\n",
      "    Batch 21 of 468 loss: 0.003600889816880226\n",
      "    Batch 22 of 468 loss: 0.0\n",
      "    Batch 23 of 468 loss: 0.0\n",
      "    Batch 24 of 468 loss: 0.017111245542764664\n",
      "    Batch 25 of 468 loss: 0.0\n",
      "    Batch 26 of 468 loss: 0.0\n",
      "    Batch 27 of 468 loss: 0.00732582900673151\n",
      "    Batch 28 of 468 loss: 0.002897080034017563\n",
      "    Batch 29 of 468 loss: 0.0\n",
      "    Batch 30 of 468 loss: 0.0035713743418455124\n",
      "    Batch 31 of 468 loss: 0.0\n",
      "    Batch 32 of 468 loss: 0.005875881761312485\n",
      "    Batch 33 of 468 loss: 0.0\n",
      "    Batch 34 of 468 loss: 0.0\n",
      "    Batch 35 of 468 loss: 0.0\n",
      "    Batch 36 of 468 loss: 0.003908680751919746\n",
      "    Batch 37 of 468 loss: 0.0\n",
      "    Batch 38 of 468 loss: 0.0\n",
      "    Batch 39 of 468 loss: 0.0\n",
      "    Batch 40 of 468 loss: 0.0\n",
      "    Batch 41 of 468 loss: 0.0\n",
      "    Batch 42 of 468 loss: 0.0\n",
      "    Batch 43 of 468 loss: 0.0\n",
      "    Batch 44 of 468 loss: 0.0025467611849308014\n",
      "    Batch 45 of 468 loss: 0.0\n",
      "    Batch 46 of 468 loss: 0.0\n",
      "    Batch 47 of 468 loss: 0.0\n",
      "    Batch 48 of 468 loss: 0.0\n",
      "    Batch 49 of 468 loss: 0.008825465105473995\n",
      "    Batch 50 of 468 loss: 0.0022475970908999443\n",
      "    Batch 51 of 468 loss: 0.0\n",
      "    Batch 52 of 468 loss: 0.001731090247631073\n",
      "    Batch 53 of 468 loss: 0.0\n",
      "    Batch 54 of 468 loss: 0.0018858369439840317\n",
      "    Batch 55 of 468 loss: 0.007383403368294239\n",
      "    Batch 56 of 468 loss: 0.0010590516030788422\n",
      "    Batch 57 of 468 loss: 0.003955986350774765\n",
      "    Batch 58 of 468 loss: 0.016582397744059563\n",
      "    Batch 59 of 468 loss: 0.014778070151805878\n",
      "    Batch 60 of 468 loss: 0.0015917178243398666\n",
      "    Batch 61 of 468 loss: 0.005210229195654392\n",
      "    Batch 62 of 468 loss: 0.0\n",
      "    Batch 63 of 468 loss: 0.0\n",
      "    Batch 64 of 468 loss: 0.0\n",
      "    Batch 65 of 468 loss: 0.0011667702347040176\n",
      "    Batch 66 of 468 loss: 0.0\n",
      "    Batch 67 of 468 loss: 0.0\n",
      "    Batch 68 of 468 loss: 0.0\n",
      "    Batch 69 of 468 loss: 0.0\n",
      "    Batch 70 of 468 loss: 0.0\n",
      "    Batch 71 of 468 loss: 0.0047671254724264145\n",
      "    Batch 72 of 468 loss: 0.0016048979014158249\n",
      "    Batch 73 of 468 loss: 0.0\n",
      "    Batch 74 of 468 loss: 0.011109361425042152\n",
      "    Batch 75 of 468 loss: 0.0\n",
      "    Batch 76 of 468 loss: 0.0033416617661714554\n",
      "    Batch 77 of 468 loss: 0.0\n",
      "    Batch 78 of 468 loss: 0.0\n",
      "    Batch 79 of 468 loss: 0.005082752555608749\n",
      "    Batch 80 of 468 loss: 0.001412179321050644\n",
      "    Batch 81 of 468 loss: 0.007110171020030975\n",
      "    Batch 82 of 468 loss: 0.0\n",
      "    Batch 83 of 468 loss: 0.0\n",
      "    Batch 84 of 468 loss: 0.0011454802006483078\n",
      "    Batch 85 of 468 loss: 0.0\n",
      "    Batch 86 of 468 loss: 0.0\n",
      "    Batch 87 of 468 loss: 0.0\n",
      "    Batch 88 of 468 loss: 0.002651641145348549\n",
      "    Batch 89 of 468 loss: 0.0\n",
      "    Batch 90 of 468 loss: 0.0\n",
      "    Batch 91 of 468 loss: 0.0022005699574947357\n",
      "    Batch 92 of 468 loss: 0.0\n",
      "    Batch 93 of 468 loss: 0.0\n",
      "    Batch 94 of 468 loss: 0.0\n",
      "    Batch 95 of 468 loss: 0.0\n",
      "    Batch 96 of 468 loss: 0.0003959629684686661\n",
      "    Batch 97 of 468 loss: 0.0\n",
      "    Batch 98 of 468 loss: 0.0\n",
      "    Batch 99 of 468 loss: 0.009809929877519608\n",
      "    Batch 100 of 468 loss: 0.0\n",
      "    Batch 101 of 468 loss: 0.0\n",
      "    Batch 102 of 468 loss: 0.0\n",
      "    Batch 103 of 468 loss: 0.00013206899166107178\n",
      "    Batch 104 of 468 loss: 0.0\n",
      "    Batch 105 of 468 loss: 0.0\n",
      "    Batch 106 of 468 loss: 0.0\n",
      "    Batch 107 of 468 loss: 0.0\n",
      "    Batch 108 of 468 loss: 0.0\n",
      "    Batch 109 of 468 loss: 0.0\n",
      "    Batch 110 of 468 loss: 0.0\n",
      "    Batch 111 of 468 loss: 0.0047033317387104034\n",
      "    Batch 112 of 468 loss: 0.0\n",
      "    Batch 113 of 468 loss: 0.0\n",
      "    Batch 114 of 468 loss: 0.0\n",
      "    Batch 115 of 468 loss: 0.0020932406187057495\n",
      "    Batch 116 of 468 loss: 0.0\n",
      "    Batch 117 of 468 loss: 0.0\n",
      "    Batch 118 of 468 loss: 0.0\n",
      "    Batch 119 of 468 loss: 0.007078677415847778\n",
      "    Batch 120 of 468 loss: 0.00011544302105903625\n",
      "    Batch 121 of 468 loss: 0.0\n",
      "    Batch 122 of 468 loss: 0.0\n",
      "    Batch 123 of 468 loss: 0.0\n",
      "    Batch 124 of 468 loss: 0.0\n",
      "    Batch 125 of 468 loss: 0.004583243280649185\n",
      "    Batch 126 of 468 loss: 8.285418152809143e-05\n",
      "    Batch 127 of 468 loss: 0.0\n",
      "    Batch 128 of 468 loss: 0.0\n",
      "    Batch 129 of 468 loss: 0.0\n",
      "    Batch 130 of 468 loss: 0.0\n",
      "    Batch 131 of 468 loss: 0.0\n",
      "    Batch 132 of 468 loss: 0.0\n",
      "    Batch 133 of 468 loss: 0.0013439003378152847\n",
      "    Batch 134 of 468 loss: 0.0\n",
      "    Batch 135 of 468 loss: 0.0\n",
      "    Batch 136 of 468 loss: 0.0\n",
      "    Batch 137 of 468 loss: 0.0\n",
      "    Batch 138 of 468 loss: 0.0\n",
      "    Batch 139 of 468 loss: 0.0\n",
      "    Batch 140 of 468 loss: 0.0\n",
      "    Batch 141 of 468 loss: 0.0\n",
      "    Batch 142 of 468 loss: 0.0\n",
      "    Batch 143 of 468 loss: 0.0\n",
      "    Batch 144 of 468 loss: 0.0\n",
      "    Batch 145 of 468 loss: 0.0\n",
      "    Batch 146 of 468 loss: 0.0\n",
      "    Batch 147 of 468 loss: 0.0\n",
      "    Batch 148 of 468 loss: 0.0\n",
      "    Batch 149 of 468 loss: 0.0\n",
      "    Batch 150 of 468 loss: 0.0\n",
      "    Batch 151 of 468 loss: 0.0\n",
      "    Batch 152 of 468 loss: 0.0\n",
      "    Batch 153 of 468 loss: 0.0\n",
      "    Batch 154 of 468 loss: 0.0\n",
      "    Batch 155 of 468 loss: 0.0\n",
      "    Batch 156 of 468 loss: 0.0\n",
      "    Batch 157 of 468 loss: 0.0\n",
      "    Batch 158 of 468 loss: 0.0\n",
      "    Batch 159 of 468 loss: 0.0\n",
      "    Batch 160 of 468 loss: 0.0\n",
      "    Batch 161 of 468 loss: 0.0\n",
      "    Batch 162 of 468 loss: 3.6017969250679016e-05\n",
      "    Batch 163 of 468 loss: 0.0\n",
      "    Batch 164 of 468 loss: 0.00018002651631832123\n",
      "    Batch 165 of 468 loss: 0.0\n",
      "    Batch 166 of 468 loss: 0.0\n",
      "    Batch 167 of 468 loss: 0.0\n",
      "    Batch 168 of 468 loss: 0.0\n",
      "    Batch 169 of 468 loss: 0.0017119888216257095\n",
      "    Batch 170 of 468 loss: 0.0\n",
      "    Batch 171 of 468 loss: 0.0\n",
      "    Batch 172 of 468 loss: 0.0\n",
      "    Batch 173 of 468 loss: 0.0014077648520469666\n",
      "    Batch 174 of 468 loss: 0.005047664977610111\n",
      "    Batch 175 of 468 loss: 0.0\n",
      "    Batch 176 of 468 loss: 0.0\n",
      "    Batch 177 of 468 loss: 0.0\n",
      "    Batch 178 of 468 loss: 0.0\n",
      "    Batch 179 of 468 loss: 0.0\n",
      "    Batch 180 of 468 loss: 0.0\n",
      "    Batch 181 of 468 loss: 0.0\n",
      "    Batch 182 of 468 loss: 0.0\n",
      "    Batch 183 of 468 loss: 0.0004422999918460846\n",
      "    Batch 184 of 468 loss: 0.0\n",
      "    Batch 185 of 468 loss: 0.0\n",
      "    Batch 186 of 468 loss: 0.0\n",
      "    Batch 187 of 468 loss: 0.0\n",
      "    Batch 188 of 468 loss: 0.0\n",
      "    Batch 189 of 468 loss: 0.0\n",
      "    Batch 190 of 468 loss: 0.0\n",
      "    Batch 191 of 468 loss: 0.0\n",
      "    Batch 192 of 468 loss: 0.005715910345315933\n",
      "    Batch 193 of 468 loss: 0.0\n",
      "    Batch 194 of 468 loss: 0.0\n",
      "    Batch 195 of 468 loss: 0.004682708531618118\n",
      "    Batch 196 of 468 loss: 0.0\n",
      "    Batch 197 of 468 loss: 0.0\n",
      "    Batch 198 of 468 loss: 0.00016046315431594849\n",
      "    Batch 199 of 468 loss: 0.006725087761878967\n",
      "    Batch 200 of 468 loss: 0.0\n",
      "    Batch 201 of 468 loss: 0.0\n",
      "    Batch 202 of 468 loss: 0.0\n",
      "    Batch 203 of 468 loss: 0.0\n",
      "    Batch 204 of 468 loss: 0.0\n",
      "    Batch 205 of 468 loss: 0.0\n",
      "    Batch 206 of 468 loss: 0.0\n",
      "    Batch 207 of 468 loss: 0.0\n",
      "    Batch 208 of 468 loss: 0.0\n",
      "    Batch 209 of 468 loss: 0.0\n",
      "    Batch 210 of 468 loss: 0.0\n",
      "    Batch 211 of 468 loss: 0.009886747226119041\n",
      "    Batch 212 of 468 loss: 0.003919094800949097\n",
      "    Batch 213 of 468 loss: 0.0\n",
      "    Batch 214 of 468 loss: 0.0\n",
      "    Batch 215 of 468 loss: 0.0\n",
      "    Batch 216 of 468 loss: 0.0\n",
      "    Batch 217 of 468 loss: 0.0\n",
      "    Batch 218 of 468 loss: 0.0\n",
      "    Batch 219 of 468 loss: 0.0\n",
      "    Batch 220 of 468 loss: 0.0\n",
      "    Batch 221 of 468 loss: 0.0\n",
      "    Batch 222 of 468 loss: 0.0\n",
      "    Batch 223 of 468 loss: 0.0\n",
      "    Batch 224 of 468 loss: 0.0\n",
      "    Batch 225 of 468 loss: 0.0015981737524271011\n",
      "    Batch 226 of 468 loss: 0.0\n",
      "    Batch 227 of 468 loss: 0.0\n",
      "    Batch 228 of 468 loss: 0.0\n",
      "    Batch 229 of 468 loss: 0.0\n",
      "    Batch 230 of 468 loss: 0.0\n",
      "    Batch 231 of 468 loss: 0.0\n",
      "    Batch 232 of 468 loss: 0.0\n",
      "    Batch 233 of 468 loss: 0.0\n",
      "    Batch 234 of 468 loss: 0.0\n",
      "    Batch 235 of 468 loss: 0.005895748734474182\n",
      "    Batch 236 of 468 loss: 0.0\n",
      "    Batch 237 of 468 loss: 0.0\n",
      "    Batch 238 of 468 loss: 0.0\n",
      "    Batch 239 of 468 loss: 0.0\n",
      "    Batch 240 of 468 loss: 0.0\n",
      "    Batch 241 of 468 loss: 0.0\n",
      "    Batch 242 of 468 loss: 0.0\n",
      "    Batch 243 of 468 loss: 0.0\n",
      "    Batch 244 of 468 loss: 0.007022146135568619\n",
      "    Batch 245 of 468 loss: 0.0\n",
      "    Batch 246 of 468 loss: 0.0\n",
      "    Batch 247 of 468 loss: 0.0\n",
      "    Batch 248 of 468 loss: 0.0007421709597110748\n",
      "    Batch 249 of 468 loss: 0.0\n",
      "    Batch 250 of 468 loss: 0.0\n",
      "    Batch 251 of 468 loss: 0.0\n",
      "    Batch 252 of 468 loss: 0.0\n",
      "    Batch 253 of 468 loss: 0.0\n",
      "    Batch 254 of 468 loss: 0.0\n",
      "    Batch 255 of 468 loss: 0.0\n",
      "    Batch 256 of 468 loss: 0.0\n",
      "    Batch 257 of 468 loss: 0.0\n",
      "    Batch 258 of 468 loss: 0.008829982951283455\n",
      "    Batch 259 of 468 loss: 0.0\n",
      "    Batch 260 of 468 loss: 0.0\n",
      "    Batch 261 of 468 loss: 0.0\n",
      "    Batch 262 of 468 loss: 0.0\n",
      "    Batch 263 of 468 loss: 0.0\n",
      "    Batch 264 of 468 loss: 0.0\n",
      "    Batch 265 of 468 loss: 0.0\n",
      "    Batch 266 of 468 loss: 0.0\n",
      "    Batch 267 of 468 loss: 0.0\n",
      "    Batch 268 of 468 loss: 0.0\n",
      "    Batch 269 of 468 loss: 0.0\n",
      "    Batch 270 of 468 loss: 0.0\n",
      "    Batch 271 of 468 loss: 0.0\n",
      "    Batch 272 of 468 loss: 0.003221796825528145\n",
      "    Batch 273 of 468 loss: 0.0\n",
      "    Batch 274 of 468 loss: 0.0\n",
      "    Batch 275 of 468 loss: 0.0\n",
      "    Batch 276 of 468 loss: 0.0\n",
      "    Batch 277 of 468 loss: 0.0\n",
      "    Batch 278 of 468 loss: 0.0\n",
      "    Batch 279 of 468 loss: 0.0\n",
      "    Batch 280 of 468 loss: 0.0\n",
      "    Batch 281 of 468 loss: 0.0\n",
      "    Batch 282 of 468 loss: 0.0\n",
      "    Batch 283 of 468 loss: 0.0\n",
      "    Batch 284 of 468 loss: 0.0\n",
      "    Batch 285 of 468 loss: 0.0\n",
      "    Batch 286 of 468 loss: 0.0\n",
      "    Batch 287 of 468 loss: 0.0\n",
      "    Batch 288 of 468 loss: 0.0\n",
      "    Batch 289 of 468 loss: 0.0\n",
      "    Batch 290 of 468 loss: 0.0\n",
      "    Batch 291 of 468 loss: 0.0\n",
      "    Batch 292 of 468 loss: 0.0\n",
      "    Batch 293 of 468 loss: 0.0\n",
      "    Batch 294 of 468 loss: 0.0\n",
      "    Batch 295 of 468 loss: 0.0\n",
      "    Batch 296 of 468 loss: 0.003333473578095436\n",
      "    Batch 297 of 468 loss: 0.0\n",
      "    Batch 298 of 468 loss: 0.010786499828100204\n",
      "    Batch 299 of 468 loss: 0.0\n",
      "    Batch 300 of 468 loss: 0.0\n",
      "    Batch 301 of 468 loss: 0.0\n",
      "    Batch 302 of 468 loss: 0.0\n",
      "    Batch 303 of 468 loss: 0.0\n",
      "    Batch 304 of 468 loss: 0.0\n",
      "    Batch 305 of 468 loss: 0.0\n",
      "    Batch 306 of 468 loss: 0.0\n",
      "    Batch 307 of 468 loss: 0.0\n",
      "    Batch 308 of 468 loss: 0.0\n",
      "    Batch 309 of 468 loss: 0.0\n",
      "    Batch 310 of 468 loss: 0.0\n",
      "    Batch 311 of 468 loss: 0.0\n",
      "    Batch 312 of 468 loss: 0.0\n",
      "    Batch 313 of 468 loss: 0.0\n",
      "    Batch 314 of 468 loss: 0.0\n",
      "    Batch 315 of 468 loss: 0.0\n",
      "    Batch 316 of 468 loss: 0.005743380635976791\n",
      "    Batch 317 of 468 loss: 0.0\n",
      "    Batch 318 of 468 loss: 0.0\n",
      "    Batch 319 of 468 loss: 0.0\n",
      "    Batch 320 of 468 loss: 0.0\n",
      "    Batch 321 of 468 loss: 0.0\n",
      "    Batch 322 of 468 loss: 0.0\n",
      "    Batch 323 of 468 loss: 0.0\n",
      "    Batch 324 of 468 loss: 0.0\n",
      "    Batch 325 of 468 loss: 0.0\n",
      "    Batch 326 of 468 loss: 0.0\n",
      "    Batch 327 of 468 loss: 0.0\n",
      "    Batch 328 of 468 loss: 0.0\n",
      "    Batch 329 of 468 loss: 0.0\n",
      "    Batch 330 of 468 loss: 0.0\n",
      "    Batch 331 of 468 loss: 0.0\n",
      "    Batch 332 of 468 loss: 0.0\n",
      "    Batch 333 of 468 loss: 0.0\n",
      "    Batch 334 of 468 loss: 0.0\n",
      "    Batch 335 of 468 loss: 0.0\n",
      "    Batch 336 of 468 loss: 0.0\n",
      "    Batch 337 of 468 loss: 0.0\n",
      "    Batch 338 of 468 loss: 0.0\n",
      "    Batch 339 of 468 loss: 0.0\n",
      "    Batch 340 of 468 loss: 0.0\n",
      "    Batch 341 of 468 loss: 0.0\n",
      "    Batch 342 of 468 loss: 0.0\n",
      "    Batch 343 of 468 loss: 0.0\n",
      "    Batch 344 of 468 loss: 0.0\n",
      "    Batch 345 of 468 loss: 0.0\n",
      "    Batch 346 of 468 loss: 0.0\n",
      "    Batch 347 of 468 loss: 0.0\n",
      "    Batch 348 of 468 loss: 0.0\n",
      "    Batch 349 of 468 loss: 0.0\n",
      "    Batch 350 of 468 loss: 0.0\n",
      "    Batch 351 of 468 loss: 0.0009988080710172653\n",
      "    Batch 352 of 468 loss: 0.0\n",
      "    Batch 353 of 468 loss: 0.0\n",
      "    Batch 354 of 468 loss: 0.0004582460969686508\n",
      "    Batch 355 of 468 loss: 0.0003627017140388489\n",
      "    Batch 356 of 468 loss: 0.0\n",
      "    Batch 357 of 468 loss: 0.0\n",
      "    Batch 358 of 468 loss: 0.0\n",
      "    Batch 359 of 468 loss: 0.0\n",
      "    Batch 360 of 468 loss: 0.0\n",
      "    Batch 361 of 468 loss: 0.012021210975944996\n",
      "    Batch 362 of 468 loss: 0.0\n",
      "    Batch 363 of 468 loss: 0.0026837140321731567\n",
      "    Batch 364 of 468 loss: 0.0\n",
      "    Batch 365 of 468 loss: 0.0\n",
      "    Batch 366 of 468 loss: 0.0\n",
      "    Batch 367 of 468 loss: 0.0\n",
      "    Batch 368 of 468 loss: 0.0\n",
      "    Batch 369 of 468 loss: 0.0\n",
      "    Batch 370 of 468 loss: 0.0\n",
      "    Batch 371 of 468 loss: 0.0\n",
      "    Batch 372 of 468 loss: 0.0\n",
      "    Batch 373 of 468 loss: 0.01728498563170433\n",
      "    Batch 374 of 468 loss: 0.0\n",
      "    Batch 375 of 468 loss: 0.0\n",
      "    Batch 376 of 468 loss: 0.0\n",
      "    Batch 377 of 468 loss: 0.0\n",
      "    Batch 378 of 468 loss: 0.0\n",
      "    Batch 379 of 468 loss: 0.0\n",
      "    Batch 380 of 468 loss: 0.0\n",
      "    Batch 381 of 468 loss: 0.0\n",
      "    Batch 382 of 468 loss: 0.0\n",
      "    Batch 383 of 468 loss: 0.008656907826662064\n",
      "    Batch 384 of 468 loss: 0.0\n",
      "    Batch 385 of 468 loss: 5.8494508266448975e-05\n",
      "    Batch 386 of 468 loss: 0.0\n",
      "    Batch 387 of 468 loss: 0.0\n",
      "    Batch 388 of 468 loss: 0.00863523967564106\n",
      "    Batch 389 of 468 loss: 0.0\n",
      "    Batch 390 of 468 loss: 0.0\n",
      "    Batch 391 of 468 loss: 0.0\n",
      "    Batch 392 of 468 loss: 0.0\n",
      "    Batch 393 of 468 loss: 0.0\n",
      "    Batch 394 of 468 loss: 0.0\n",
      "    Batch 395 of 468 loss: 0.0\n",
      "    Batch 396 of 468 loss: 0.0\n",
      "    Batch 397 of 468 loss: 0.0\n",
      "    Batch 398 of 468 loss: 0.0\n",
      "    Batch 399 of 468 loss: 0.0\n",
      "    Batch 400 of 468 loss: 0.0\n",
      "    Batch 401 of 468 loss: 0.0\n",
      "    Batch 402 of 468 loss: 0.005181066691875458\n",
      "    Batch 403 of 468 loss: 0.0\n",
      "    Batch 404 of 468 loss: 0.0\n",
      "    Batch 405 of 468 loss: 0.0\n",
      "    Batch 406 of 468 loss: 0.0\n",
      "    Batch 407 of 468 loss: 0.0\n",
      "    Batch 408 of 468 loss: 0.006299681030213833\n",
      "    Batch 409 of 468 loss: 0.0\n",
      "    Batch 410 of 468 loss: 0.0030972734093666077\n",
      "    Batch 411 of 468 loss: 0.0\n",
      "    Batch 412 of 468 loss: 0.0\n",
      "    Batch 413 of 468 loss: 0.0\n",
      "    Batch 414 of 468 loss: 0.0\n",
      "    Batch 415 of 468 loss: 0.0\n",
      "    Batch 416 of 468 loss: 0.0\n",
      "    Batch 417 of 468 loss: 0.0\n",
      "    Batch 418 of 468 loss: 0.0\n",
      "    Batch 419 of 468 loss: 0.0\n",
      "    Batch 420 of 468 loss: 0.0\n",
      "    Batch 421 of 468 loss: 0.0\n",
      "    Batch 422 of 468 loss: 0.0\n",
      "    Batch 423 of 468 loss: 0.0\n",
      "    Batch 424 of 468 loss: 0.0\n",
      "    Batch 425 of 468 loss: 0.0\n",
      "    Batch 426 of 468 loss: 0.0\n",
      "    Batch 427 of 468 loss: 0.0006292723119258881\n",
      "    Batch 428 of 468 loss: 0.0\n",
      "    Batch 429 of 468 loss: 0.0012845024466514587\n",
      "    Batch 430 of 468 loss: 0.001511717215180397\n",
      "    Batch 431 of 468 loss: 0.002713855355978012\n",
      "    Batch 432 of 468 loss: 0.0\n",
      "    Batch 433 of 468 loss: 0.0\n",
      "    Batch 434 of 468 loss: 0.0\n",
      "    Batch 435 of 468 loss: 0.0\n",
      "    Batch 436 of 468 loss: 0.0\n",
      "    Batch 437 of 468 loss: 0.0\n",
      "    Batch 438 of 468 loss: 0.0\n",
      "    Batch 439 of 468 loss: 0.0\n",
      "    Batch 440 of 468 loss: 0.0\n",
      "    Batch 441 of 468 loss: 0.0\n",
      "    Batch 442 of 468 loss: 0.0\n",
      "    Batch 443 of 468 loss: 0.0\n",
      "    Batch 444 of 468 loss: 0.0\n",
      "    Batch 445 of 468 loss: 0.0\n",
      "    Batch 446 of 468 loss: 0.0\n",
      "    Batch 447 of 468 loss: 0.0\n",
      "    Batch 448 of 468 loss: 0.0\n",
      "    Batch 449 of 468 loss: 0.0\n",
      "    Batch 450 of 468 loss: 0.0\n",
      "    Batch 451 of 468 loss: 0.0\n",
      "    Batch 452 of 468 loss: 0.0\n",
      "    Batch 453 of 468 loss: 0.0\n",
      "    Batch 454 of 468 loss: 0.0\n",
      "    Batch 455 of 468 loss: 0.0\n",
      "    Batch 456 of 468 loss: 0.0017059259116649628\n",
      "    Batch 457 of 468 loss: 0.0\n",
      "    Batch 458 of 468 loss: 0.0\n",
      "    Batch 459 of 468 loss: 0.0\n",
      "    Batch 460 of 468 loss: 0.0\n",
      "    Batch 461 of 468 loss: 0.0\n",
      "    Batch 462 of 468 loss: 0.0\n",
      "    Batch 463 of 468 loss: 0.0\n",
      "    Batch 464 of 468 loss: 0.0\n",
      "    Batch 465 of 468 loss: 0.0\n",
      "    Batch 466 of 468 loss: 0.0\n",
      "    Batch 467 of 468 loss: 0.0\n",
      "    Batch 468 of 468 loss: 0.0\n",
      "Epoch: 0  |  train loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "/mnt/backup/ml/audio/venv/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "encoder = train_encoder()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from dataset import AudioSamplePairDataset\n",
    "from train import transform_sample\n",
    "test_data = AudioSamplePairDataset(root_path='data/fma_small_samples', test=False, transform=transform_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "t = [test_data[i][0] for i in range(1000)]\n",
    "x = test_data[20][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import torch\n",
    "t_s = torch.stack(t)\n",
    "x_s = x.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "t_e = encoder(t_s.cuda()).cpu()\n",
    "x_e = encoder(x_s.cuda()).cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "dists = (x_e - t_e).abs().sum(dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.1731], grad_fn=<SumBackward1>)\n",
      "tensor(10.1731, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print((x_e - t_e[20]).abs().sum(dim=-1))\n",
    "print(dists.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}